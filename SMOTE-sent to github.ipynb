{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b33e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE, ADASYN,SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report,matthews_corrcoef\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# path to the dataset\n",
    "# file = 'Shaleeza.Dataset.v1.csv'\n",
    "file = 'IoT Network Intrusion Dataset.csv'\n",
    "data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af7c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(targets_others):\n",
    "    dataset = pd.read_csv(file, error_bad_lines=False, low_memory=False)\n",
    "    dataset = dataset.drop(['Flow_ID', 'Src_IP', 'Dst_IP', 'Dst_Port', 'Protocol'], axis=1)\n",
    "    dataset = dataset.drop(['Timestamp'], axis=1)\n",
    "\n",
    "    # contain only single values\n",
    "    dataset = dataset.drop(\n",
    "        ['Fwd_PSH_Flags', 'Fwd_URG_Flags', 'Fwd_Byts/b_Avg', 'Fwd_Pkts/b_Avg', 'Fwd_Blk_Rate_Avg', 'Bwd_Byts/b_Avg',\n",
    "         'Bwd_Pkts/b_Avg', 'Bwd_Blk_Rate_Avg', 'Init_Fwd_Win_Byts', 'Fwd_Seg_Size_Min'], axis=1)\n",
    "\n",
    "    dataset['Flow_Byts/s'] = round(dataset['Flow_Byts/s'],2)\n",
    "\n",
    "    dataset = dataset.drop(targets_others, axis=1)\n",
    "\n",
    "    dataset = dataset.reset_index()\n",
    "    dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    dataset.dropna(inplace=True)\n",
    "\n",
    "    # correlation\n",
    "    correlated_features = set()\n",
    "    correlation_matrix = dataset.corr()\n",
    "\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) >= 0.7:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "\n",
    "    dataset.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# def kFoldCV(model, data, n_fold=10):\n",
    "#     diff = int(len(data)/n_fold)\n",
    "#     results = np.zeros((1, 4))\n",
    "#     predictY = deepcopy(data[:,-1])\n",
    "#     for i in range(n_fold):\n",
    "#         begin = diff*i\n",
    "#         end = diff*(i+1)\n",
    "# #         if i == n_fold-1:\n",
    "# #             end = -1\n",
    "#         test = data[begin:end]\n",
    "#         train = deepcopy(data)\n",
    "#         train = np.delete(train, range(begin, end),axis=0)\n",
    "#         X_train, y_train = SMOTE().fit_resample(train[:,:-1], train[:,-1])\n",
    "#         predictY[begin:end] = model.fit(X_train, y_train).predict(test[:,:-1])\n",
    "#     t = classification_report(data[:,-1], predictY)\n",
    "#     print(t)\n",
    "#     print(matthews_corrcoef(data[:,-1], predictY))\n",
    "        \n",
    "# #         results = results + getResults(model, train[:,:-1], train[:,-1],test[:,:-1],test[:,-1])\n",
    "# #     return results/n_fold\n",
    "def kFoldCV(model, data, n_fold=10):\n",
    "    diff = int(len(data)/n_fold)\n",
    "    results = np.zeros((1, 4))\n",
    "    predictY = data[:,-1].astype('int')\n",
    "    targetY = deepcopy(predictY).astype('int')\n",
    "#     predictY = deepcopy(data[:,-1]).astype('int')\n",
    "    cv = StratifiedKFold(n_splits=n_fold)\n",
    "    X, y = data[:,:-1],data[:,-1].astype('int')\n",
    "    begin = 0\n",
    "    for fold, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "#         print(X_train.shape,y[test_index].shape)\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "        X_test = sc.transform(X[test_index])  \n",
    "        predictY[begin:begin+len(X_test)] = model.fit(X_train, y_train).predict(X_test)\n",
    "        targetY[begin:begin+len(X_test)] = y[test_index]\n",
    "        begin += len(X_test) \n",
    "#         targetY[begin:end] = test[:,-1]\n",
    "#         predictY[begin:end] = model.fit(X_train, y_train).predict(X_test)\n",
    "    t = classification_report(targetY, predictY)\n",
    "    print(t)\n",
    "    print(matthews_corrcoef(targetY, predictY))\n",
    "\n",
    "def getResults(model, X_train, y_train,X_test,y):\n",
    "    predictY = model.fit(X_train, y_train).predict(X_test)\n",
    "    t = classification_report(y, predictY)#, target_names=['0', '1', '2']\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04ddc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13512\\857162991.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  dataset = pd.read_csv(file, error_bad_lines=False, low_memory=False)\n"
     ]
    }
   ],
   "source": [
    "data = data_preprocessing(['Label', 'Cat'])\n",
    "data = data.values\n",
    "data = data[:,1:]\n",
    "X_train, y_train = data[:,:-1], data[:,-1]\n",
    "y_train = LabelEncoder().fit_transform(y_train)\n",
    "\n",
    "data = np.c_[X_train,y_train.astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyper-parameter tuning, after findin the optimal hyper-parameters, they are used for the final training and prediction\n",
    "t = time()\n",
    "length = [3, 4, 5]\n",
    "for i in length:\n",
    "    model = DecisionTreeClassifier(max_depth=i)\n",
    "    kFoldCV(model, data)\n",
    "\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5ad66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59391\n",
      "           1       0.28      0.41      0.33     35377\n",
      "           2       0.00      0.00      0.00     55124\n",
      "           3       0.25      0.86      0.39     55818\n",
      "           4       0.61      0.30      0.40    121178\n",
      "           5       1.00      0.59      0.74    183189\n",
      "           6       1.00      0.61      0.76     40073\n",
      "           7       0.00      0.00      0.00     22192\n",
      "           8       0.38      0.95      0.54     53073\n",
      "\n",
      "    accuracy                           0.54    625415\n",
      "   macro avg       0.50      0.52      0.46    625415\n",
      "weighted avg       0.64      0.54      0.54    625415\n",
      "\n",
      "0.5036565209743296\n",
      "129.37770175933838\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "length = [5]\n",
    "for i in length:\n",
    "    model = DecisionTreeClassifier(max_depth=i)\n",
    "    kFoldCV(model, data)\n",
    "\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyper-parameter tuning, after findin the optimal hyper-parameters, they are used for the final training and prediction\n",
    "\n",
    "t = time()\n",
    "hidden_layer_sizes  = [100, 200, 300]\n",
    "max_iter = [100, 200, 300]\n",
    "for i in hidden_layer_sizes:\n",
    "    for j in max_iter:\n",
    "        model = MLPClassifier(hidden_layer_sizes=i, max_iter=j)\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c387c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59391\n",
      "           1       0.72      0.91      0.80     35377\n",
      "           2       0.32      0.49      0.38     55124\n",
      "           3       0.32      0.47      0.38     55818\n",
      "           4       0.91      0.73      0.81    121178\n",
      "           5       1.00      0.70      0.82    183189\n",
      "           6       0.91      0.91      0.91     40073\n",
      "           7       0.27      0.47      0.34     22192\n",
      "           8       0.65      0.62      0.63     53073\n",
      "\n",
      "    accuracy                           0.71    625415\n",
      "   macro avg       0.68      0.70      0.68    625415\n",
      "weighted avg       0.78      0.71      0.73    625415\n",
      "\n",
      "0.6627325060427195\n",
      "34490.25239944458\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "hidden_layer_sizes  = [100, 200, 300]\n",
    "max_iter = [100, 200, 300]\n",
    "hidden_layer_sizes  = [300]\n",
    "max_iter = [300]\n",
    "for i in hidden_layer_sizes:\n",
    "    for j in max_iter:\n",
    "        model = MLPClassifier(hidden_layer_sizes=i, max_iter=j)\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyper-parameter tuning, after findin the optimal hyper-parameters, they are used for the final training and prediction\n",
    "t = time()\n",
    "n_estimators  = [10, 100, 200]\n",
    "max_depth = [3, 4, 5]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        model = RandomForestClassifier(n_estimators=i, max_depth=j)\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c194c394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59391\n",
      "           1       0.32      0.88      0.47     35377\n",
      "           2       0.30      0.44      0.36     55124\n",
      "           3       0.31      0.41      0.35     55818\n",
      "           4       0.89      0.22      0.35    121178\n",
      "           5       0.98      0.71      0.82    183189\n",
      "           6       0.80      0.85      0.82     40073\n",
      "           7       0.18      0.14      0.16     22192\n",
      "           8       0.47      0.83      0.60     53073\n",
      "\n",
      "    accuracy                           0.60    625415\n",
      "   macro avg       0.58      0.61      0.55    625415\n",
      "weighted avg       0.72      0.60      0.60    625415\n",
      "\n",
      "0.5528415274379004\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39mi, max_depth\u001b[38;5;241m=\u001b[39mj)\n\u001b[0;32m      8\u001b[0m         kFoldCV(model, data)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(time()\u001b[38;5;241m-\u001b[39m\u001b[43mt\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "n_estimators  = [10, 100, 200]\n",
    "max_depth = [3, 4, 5]\n",
    "n_estimators  = [100]\n",
    "max_depth = [5]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        model = RandomForestClassifier(n_estimators=i, max_depth=j)\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "print(time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyper-parameter tuning, after findin the optimal hyper-parameters, they are used for the final training and prediction\n",
    "t = time()\n",
    "n_estimators  = [10, 100, 200]\n",
    "max_depth = [3, 4, 5]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        model = XGBClassifier(n_estimators=i, max_depth=j,objective='mlogloss')\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3827878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59391\n",
      "           1       0.92      0.98      0.95     35377\n",
      "           2       0.29      0.55      0.38     55124\n",
      "           3       0.28      0.31      0.29     55818\n",
      "           4       0.97      0.86      0.91    121178\n",
      "           5       1.00      0.70      0.83    183189\n",
      "           6       0.91      0.94      0.92     40073\n",
      "           7       0.49      0.60      0.54     22192\n",
      "           8       0.80      0.83      0.82     53073\n",
      "\n",
      "    accuracy                           0.75    625415\n",
      "   macro avg       0.74      0.75      0.74    625415\n",
      "weighted avg       0.82      0.75      0.77    625415\n",
      "\n",
      "0.7163350390485673\n",
      "3302.937599182129\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "n_estimators  = [10, 100, 200]\n",
    "max_depth = [3, 4, 5]\n",
    "n_estimators  = [200]\n",
    "max_depth = [5]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        model = XGBClassifier(n_estimators=i, max_depth=j,objective='mlogloss')\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldCV(model, data, n_fold=10):\n",
    "    diff = int(len(data)/n_fold)\n",
    "    results = np.zeros((1, 4))\n",
    "    predictY = deepcopy(data[:,-1]).reshape(len(data[:,-1]),1).astype('int')\n",
    "    targetY = deepcopy(data[:,-1]).reshape(len(data[:,-1]),1).astype('int')\n",
    "#     predictY = deepcopy(data[:,-1]).astype('int')\n",
    "    cv = StratifiedKFold(n_splits=n_fold)\n",
    "    X, y = data[:,:-1],data[:,-1].astype('int')\n",
    "    begin = 0\n",
    "    for fold, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
    "        y = y.astype('int').reshape(len(y),1)\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "#         print(X_train.shape,y[test_index].shape)\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "        X_test = sc.transform(X[test_index])  \n",
    "        predictY[begin:begin+len(X_test)] = model.fit(X_train, y_train).predict(X_test)\n",
    "        targetY[begin:begin+len(X_test)] = y[test_index]\n",
    "        begin += len(X_test) \n",
    "#         targetY[begin:end] = test[:,-1]\n",
    "#         predictY[begin:end] = model.fit(X_train, y_train).predict(X_test)\n",
    "    t = classification_report(targetY, predictY)\n",
    "    print(t)\n",
    "    print(matthews_corrcoef(targetY, predictY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6724a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyper-parameter tuning, after findin the optimal hyper-parameters, they are used for the final training and prediction\n",
    "t = time()\n",
    "n_estimators  = [10, 100, 200]\n",
    "max_depth = [3, 4, 5]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        model = CatBoostClassifier(n_estimators=i, max_depth=j)\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "n_estimators  = [10, 100, 200]\n",
    "max_depth = [3, 4, 5]\n",
    "n_estimators  = [200]\n",
    "max_depth = [5]\n",
    "for i in n_estimators:\n",
    "    for j in max_depth:\n",
    "        model = CatBoostClassifier(n_estimators=i, max_depth=j,verbose=False)\n",
    "        kFoldCV(model, data)\n",
    "\n",
    "print(time()-t)\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00     59391\n",
    "#            1       0.83      0.97      0.89     35377\n",
    "#            2       0.31      0.60      0.41     55124\n",
    "#            3       0.31      0.33      0.32     55818\n",
    "#            4       0.95      0.83      0.88    121178\n",
    "#            5       1.00      0.70      0.83    183189\n",
    "#            6       0.89      0.93      0.91     40073\n",
    "#            7       0.47      0.59      0.52     22192\n",
    "#            8       0.80      0.81      0.81     53073\n",
    "\n",
    "#     accuracy                           0.75    625415\n",
    "#    macro avg       0.73      0.75      0.73    625415\n",
    "# weighted avg       0.82      0.75      0.77    625415\n",
    "\n",
    "# 0.7121565931346628\n",
    "# 1555.636743068695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca06fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
